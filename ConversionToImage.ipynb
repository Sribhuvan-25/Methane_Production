{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-bio\n",
      "  Downloading scikit-bio-0.6.3.tar.gz (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20.0 in /Users/sb/miniforge3/envs/torch/lib/python3.11/site-packages (from scikit-bio) (2.32.3)\n",
      "Requirement already satisfied: decorator>=3.4.2 in /Users/sb/miniforge3/envs/torch/lib/python3.11/site-packages (from scikit-bio) (5.1.1)\n",
      "Requirement already satisfied: natsort>=4.0.3 in /Users/sb/miniforge3/envs/torch/lib/python3.11/site-packages (from scikit-bio) (8.4.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/sb/miniforge3/envs/torch/lib/python3.11/site-packages (from scikit-bio) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /Users/sb/miniforge3/envs/torch/lib/python3.11/site-packages (from scikit-bio) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.9.0 in /Users/sb/miniforge3/envs/torch/lib/python3.11/site-packages (from scikit-bio) (1.13.1)\n",
      "Requirement already satisfied: h5py>=3.6.0 in /Users/sb/miniforge3/envs/torch/lib/python3.11/site-packages (from scikit-bio) (3.11.0)\n",
      "Collecting biom-format>=2.1.16 (from scikit-bio)\n",
      "  Downloading biom-format-2.1.16.tar.gz (11.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: statsmodels>=0.14.0 in /Users/sb/miniforge3/envs/torch/lib/python3.11/site-packages (from scikit-bio) (0.14.4)\n",
      "Requirement already satisfied: patsy>=0.5.0 in /Users/sb/miniforge3/envs/torch/lib/python3.11/site-packages (from scikit-bio) (1.0.1)\n",
      "Requirement already satisfied: click in /Users/sb/miniforge3/envs/torch/lib/python3.11/site-packages (from biom-format>=2.1.16->scikit-bio) (8.1.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/sb/miniforge3/envs/torch/lib/python3.11/site-packages (from pandas>=1.5.0->scikit-bio) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/sb/miniforge3/envs/torch/lib/python3.11/site-packages (from pandas>=1.5.0->scikit-bio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/sb/miniforge3/envs/torch/lib/python3.11/site-packages (from pandas>=1.5.0->scikit-bio) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sb/miniforge3/envs/torch/lib/python3.11/site-packages (from requests>=2.20.0->scikit-bio) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sb/miniforge3/envs/torch/lib/python3.11/site-packages (from requests>=2.20.0->scikit-bio) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sb/miniforge3/envs/torch/lib/python3.11/site-packages (from requests>=2.20.0->scikit-bio) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sb/miniforge3/envs/torch/lib/python3.11/site-packages (from requests>=2.20.0->scikit-bio) (2024.6.2)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/sb/miniforge3/envs/torch/lib/python3.11/site-packages (from statsmodels>=0.14.0->scikit-bio) (24.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sb/miniforge3/envs/torch/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->scikit-bio) (1.16.0)\n",
      "Building wheels for collected packages: scikit-bio, biom-format\n",
      "  Building wheel for scikit-bio (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for scikit-bio: filename=scikit_bio-0.6.3-cp311-cp311-macosx_11_0_arm64.whl size=4805318 sha256=2a14bb106a77fff8aa9c5cde498c4230f193f5678090bb8b3c89d2fe015cc2d9\n",
      "  Stored in directory: /Users/sb/Library/Caches/pip/wheels/c9/11/a2/86492071506a487bcba7015b710a9e89d66ca69845512fb86c\n",
      "  Building wheel for biom-format (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for biom-format: filename=biom_format-2.1.16-cp311-cp311-macosx_11_0_arm64.whl size=11780615 sha256=728fd4715ad211e426efcb35c002e2e14db6db256e3b1ab637fac64a3b17deba\n",
      "  Stored in directory: /Users/sb/Library/Caches/pip/wheels/a5/6b/58/a879e8fbae2479a3d1a68719f3a062fe62701d6494f1b74f5e\n",
      "Successfully built scikit-bio biom-format\n",
      "Installing collected packages: biom-format, scikit-bio\n",
      "Successfully installed biom-format-2.1.16 scikit-bio-0.6.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: biopython in /Users/sb/miniforge3/envs/torch/lib/python3.11/site-packages (1.85)\n",
      "Requirement already satisfied: numpy in /Users/sb/miniforge3/envs/torch/lib/python3.11/site-packages (from biopython) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from Bio import Phylo\n",
    "\n",
    "##############################################################################\n",
    "# 1. LOAD / PREPARE YOUR DATA\n",
    "##############################################################################\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Assume you already have a DataFrame called df with shape: (num_samples, num_otus).\n",
    "# Rows = samples, Columns = OTUs, and the values are abundances (counts, relative abundances, etc.).\n",
    "# If you have extra metadata columns, remove or separate them so that only OTU columns remain.\n",
    "\n",
    "# Example (dummy) df creation if you need to simulate:\n",
    "# Comment this out if you already have df\n",
    "# np.random.seed(42)\n",
    "# df = pd.DataFrame(\n",
    "#     np.random.randint(0, 100, (10, 5)),  # 10 samples, 5 OTUs\n",
    "#     columns=[\"OTU_1\", \"OTU_2\", \"OTU_3\", \"OTU_4\", \"OTU_5\"]\n",
    "# )\n",
    "\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "print(\"DataFrame head:\\n\", df.head())\n",
    "\n",
    "##############################################################################\n",
    "# 2. LOAD / PARSE YOUR PHYLOGENETIC TREE\n",
    "##############################################################################\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Suppose you have a phylogenetic tree in Newick format saved in \"tree.nwk\".\n",
    "# Make sure the tip labels in the tree correspond to your OTU names in df.columns.\n",
    "\n",
    "# tree = Phylo.read(\"tree.nwk\", \"newick\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# For demonstration, we’ll create a simple example tree with Biopython:\n",
    "# You can comment this out and load your actual tree above.\n",
    "\n",
    "from io import StringIO\n",
    "example_tree_newick = \"(OTU_4:0.1,(OTU_2:0.2,(OTU_1:0.1,OTU_3:0.3):0.2):0.3,OTU_5:0.4);\"\n",
    "tree = Phylo.read(StringIO(example_tree_newick), \"newick\")\n",
    "\n",
    "print(\"\\nPhylogenetic Tree Tips (OTUs) in order:\")\n",
    "Phylo.draw_ascii(tree)\n",
    "\n",
    "##############################################################################\n",
    "# 3. DETERMINE A PHYLOGENETIC ORDER OF OTUs\n",
    "##############################################################################\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Extract tip names in a left-to-right traversal (or any consistent traversal).\n",
    "# The tip order will define the rearrangement of your columns.\n",
    "\n",
    "def get_tip_labels(tree):\n",
    "    \"\"\"Return tip labels (OTU names) from the tree in a consistent traversal order.\"\"\"\n",
    "    tip_labels = []\n",
    "    for tip in tree.get_terminals():\n",
    "        tip_labels.append(tip.name)\n",
    "    return tip_labels\n",
    "\n",
    "phylo_otus = get_tip_labels(tree)\n",
    "print(\"\\nPhylogenetic OTU order:\", phylo_otus)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Reorder the columns of df based on the tree’s tip labels\n",
    "# Only keep columns that actually appear in df (in case your tree has extra tips or vice versa).\n",
    "phylo_otus_in_df = [otu for otu in phylo_otus if otu in df.columns]\n",
    "df_ordered = df[phylo_otus_in_df]\n",
    "\n",
    "print(\"\\nDataFrame columns after reordering by phylogenetic order:\")\n",
    "print(df_ordered.columns)\n",
    "\n",
    "##############################################################################\n",
    "# 4. CONVERT THE ORDERED OTU ABUNDANCE ROW INTO A 2D \"IMAGE\"\n",
    "##############################################################################\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Many approaches exist for turning 1D vectors into 2D grids:\n",
    "#   - Simple: Reshape the vector (num_otus,) into something like sqrt(num_otus) x sqrt(num_otus)\n",
    "#   - More complex: Use pairwise distances or the actual tree structure to embed in 2D\n",
    "#\n",
    "# Here, we'll do a simplistic approach: if you have M OTUs, we make an L x L image\n",
    "# where L = int(ceil(sqrt(M))). We fill in row-major order, or zero-pad if needed.\n",
    "\n",
    "import math\n",
    "\n",
    "def vector_to_square_image(vector):\n",
    "    \"\"\"Reshape a 1D vector into the smallest square (zero-padded if necessary).\"\"\"\n",
    "    m = len(vector)\n",
    "    size = int(math.ceil(math.sqrt(m)))\n",
    "    image = np.zeros((size, size), dtype=float)\n",
    "    # Fill in row-major order\n",
    "    image.ravel()[:m] = vector\n",
    "    return image\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# We'll transform each sample's phylo-ordered OTU abundances into a 2D image.\n",
    "# If we have N samples, we get N images, each shape = (size, size).\n",
    "\n",
    "def df_to_image_array(df_phylo):\n",
    "    \"\"\"Convert each row of df_phylo into a 2D image, return a 3D array: (num_samples, size, size).\"\"\"\n",
    "    images = []\n",
    "    for idx, row in df_phylo.iterrows():\n",
    "        vec = row.values\n",
    "        img_2d = vector_to_square_image(vec)\n",
    "        images.append(img_2d)\n",
    "    return np.array(images)\n",
    "\n",
    "images_array = df_to_image_array(df_ordered)\n",
    "print(\"\\nImages array shape:\", images_array.shape)\n",
    "\n",
    "# If images_array has shape (num_samples, height, width), for a CNN we often need a channel dimension.\n",
    "# e.g. shape -> (num_samples, height, width, 1)\n",
    "images_array = np.expand_dims(images_array, axis=-1)\n",
    "print(\"Images array shape (with channel):\", images_array.shape)\n",
    "\n",
    "##############################################################################\n",
    "# 5. PREPARE LABELS (Y) FOR CLASSIFICATION OR REGRESSION\n",
    "##############################################################################\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Typically, you have some phenotype or class label per sample. For example:\n",
    "#   - a disease status (case/control)\n",
    "#   - or a numeric trait\n",
    "#\n",
    "# We'll create dummy labels for demonstration. Suppose we do a binary classification.\n",
    "\n",
    "num_samples = images_array.shape[0]\n",
    "# Dummy: label half the samples as 0, half as 1\n",
    "dummy_labels = np.array([0 if i < num_samples/2 else 1 for i in range(num_samples)])\n",
    "\n",
    "# If you have real labels in e.g. df_labels, you’d do:\n",
    "# dummy_labels = df_labels.values\n",
    "\n",
    "print(\"\\nLabels shape:\", dummy_labels.shape)\n",
    "\n",
    "##############################################################################\n",
    "# 6. SPLIT INTO TRAIN/TEST (OR TRAIN/VAL/TEST)\n",
    "##############################################################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    images_array,\n",
    "    dummy_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nTrain set X shape:\", X_train.shape, \"y shape:\", y_train.shape)\n",
    "print(\"Test set X shape:\", X_test.shape, \"y shape:\", y_test.shape)\n",
    "\n",
    "##############################################################################\n",
    "# 7. BUILD A SIMPLE CNN MODEL\n",
    "##############################################################################\n",
    "\n",
    "def create_cnn_model(input_shape):\n",
    "    \"\"\"\n",
    "    Create a simple CNN model using Keras/TF.\n",
    "    input_shape = (height, width, channels)\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=8, kernel_size=(3,3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters=16, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # for binary classification\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = create_cnn_model(input_shape=X_train.shape[1:])\n",
    "model.summary()\n",
    "\n",
    "##############################################################################\n",
    "# 8. TRAIN THE CNN\n",
    "##############################################################################\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=10,             # Increase epochs for real data\n",
    "    batch_size=2,          # Adjust batch size to your data size\n",
    "    validation_split=0.2,  # For demonstration, splits off part of training set for validation\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "##############################################################################\n",
    "# 9. EVALUATE ON TEST SET\n",
    "##############################################################################\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "##############################################################################\n",
    "# 10. PREDICT / USE THE MODEL\n",
    "##############################################################################\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"\\nPredictions (raw):\", predictions.flatten())\n",
    "print(\"Predicted classes:\", predicted_classes)\n",
    "print(\"True classes:\", y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
