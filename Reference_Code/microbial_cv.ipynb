{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.13.0\n",
      "keras version: 2.13.1\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "### Import Relevant Libraries\n",
    "'''\n",
    "import numpy as np # Version 1.16.0\n",
    "import tensorflow as tf # Version 1.12.0\n",
    "import pandas as pd\n",
    "import keras # Version 2.2.4\n",
    "from sklearn.metrics import r2_score\n",
    "import innvestigate as inn # Version 1.0.8\n",
    "print(\"tensorflow version:\", tf.__version__)\n",
    "print(\"keras version:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "### Read and Process Data\n",
    "'''\n",
    "# Load in raw count data for neural network\n",
    "X = pd.read_csv(\"Data/X.csv\", index_col=0)\n",
    "# Load in ground truth methane production rate data\n",
    "y = pd.read_csv(\"Data/Y.csv\", index_col=0)\n",
    "# Set parameters\n",
    "num_samples = X.shape[0]\n",
    "num_folds = 149 # Leave-one-out = 149\n",
    "num_features = 50 # All features = 489\n",
    "# Create linearly spaced chunks for cross validation\n",
    "chunks = np.ceil(np.linspace(0,num_samples, num=num_folds+1)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "### Define and Compile Model\n",
    "'''\n",
    "def build_model(input_shape):\n",
    "    # Define model\n",
    "    model = keras.models.Sequential([\n",
    "            keras.layers.Conv1D(filters=128, kernel_size=1, activation='relu', input_shape=input_shape),\n",
    "            keras.layers.Conv1D(filters=128, kernel_size=1, activation='relu'),\n",
    "            keras.layers.Conv1D(filters=128, kernel_size=1, activation='relu'),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            keras.layers.Conv1D(filters=128, kernel_size=1, activation='relu'),\n",
    "            keras.layers.Conv1D(filters=128, kernel_size=1, activation='relu'),\n",
    "            keras.layers.Conv1D(filters=128, kernel_size=1, activation='relu'),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            keras.layers.Conv1D(filters=64, kernel_size=1, activation='relu'),\n",
    "            keras.layers.Conv1D(filters=64, kernel_size=1, activation='relu'),\n",
    "            keras.layers.Conv1D(filters=64, kernel_size=1, activation='relu'),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            keras.layers.Conv1D(filters=64, kernel_size=1, activation='relu'),\n",
    "            keras.layers.Conv1D(filters=64, kernel_size=1, activation='relu'),\n",
    "            keras.layers.Conv1D(filters=64, kernel_size=1, activation='relu'),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            keras.layers.Conv1D(filters=32, kernel_size=1, activation='relu'),\n",
    "            keras.layers.Conv1D(filters=32, kernel_size=1, activation='relu'),\n",
    "            keras.layers.Conv1D(filters=32, kernel_size=1, activation='relu'),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            keras.layers.Conv1D(filters=32, kernel_size=1, activation='relu'),\n",
    "            keras.layers.Conv1D(filters=32, kernel_size=1, activation='relu'),\n",
    "            keras.layers.Conv1D(filters=32, kernel_size=1, activation='relu'),\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(64, activation='relu'),\n",
    "            keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    # Compile Model\n",
    "    model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=0.001))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "### Run Neural Network Cross Validation\n",
    "'''\n",
    "lrp_cache = pd.DataFrame()\n",
    "ann_predictions = np.array([])\n",
    "history_cache = []\n",
    "for fold in range(num_folds):\n",
    "\n",
    "    # Reset keras session to reduce model clutter\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Select validation samples\n",
    "    X_val = X[chunks[fold]:chunks[fold+1]]\n",
    "    y_val = y[chunks[fold]:chunks[fold+1]]\n",
    "    # Select training samples \n",
    "    X_train = X.drop(X_val.index)\n",
    "    y_train = y.drop(X_val.index).values.flatten()\n",
    "    \n",
    "    # Feature selection using Layerwise Relevance Propegation (LRP)\n",
    "    # Build and train model for LRP\n",
    "    model = build_model((X_train.shape[1],1))\n",
    "    model.fit(np.expand_dims(X_train.values, axis=2), y_train, batch_size=32, epochs=150, verbose=0)\n",
    "    # Sort features by LRP Relevance Score\n",
    "    analyzer = inn.create_analyzer(\"lrp.z_plus_fast\", model)\n",
    "    # Perform backwards pass through trained neural network to generate relevance scores\n",
    "    scores = analyzer.analyze(np.expand_dims(X_train.values, axis=2))[...,0]\n",
    "    # Store data in Dataframe\n",
    "    lrp = pd.DataFrame(scores.mean(axis=0), index=X_train.columns, columns=[\"Score\"])\n",
    "    # Sort scores by absolute value\n",
    "    lrp[\"Abs Score\"] = np.abs(lrp[\"Score\"])\n",
    "    lrp_cache[fold] = lrp[\"Abs Score\"]\n",
    "    lrp.sort_values(by=\"Abs Score\", ascending=False, inplace=True)\n",
    "\n",
    "    # Select most important features\n",
    "    X_train = X_train[lrp.index[:num_features]]\n",
    "    X_val = X_val[lrp.index[:num_features]]\n",
    "\n",
    "    # Reshape data for nerual network\n",
    "    X_train = np.asarray(X_train).reshape((X_train.shape[0],X_train.shape[1],1))\n",
    "    X_val = np.asarray(X_val).reshape((X_val.shape[0],X_val.shape[1],1))\n",
    "\n",
    "    # Run neural network model\n",
    "    model = build_model((X_train.shape[1],1))\n",
    "    history = model.fit(X_train, y_train, batch_size=32, epochs=150, verbose=0, validation_data=(X_val, y_val))\n",
    "\n",
    "    # Cache prediction values to array\n",
    "    predictions = model.predict(X_val).flatten()\n",
    "    ann_predictions = np.concatenate([ann_predictions, predictions])\n",
    "    history_cache.append(history)\n",
    "\n",
    "    # Print status update\n",
    "    print(\"--------[{}/{}]--------\".format(fold+1, num_folds))\n",
    "    for i in range(chunks[fold+1] - chunks[fold]):\n",
    "        print(\"Validation Sample:\", y.index.values[chunks[fold]+i])\n",
    "        print(\"ANN Prediction: {:.5f}\".format(ann_predictions[chunks[fold]+i]))\n",
    "        print(\"Ground Truth: {:.5f}\\n\".format(y.values.flatten()[chunks[fold]+i]))\n",
    "\n",
    "# Print results    \n",
    "print(\"\\nCross Validation Results:\\n\")\n",
    "ann_r2 = r2_score(y, ann_predictions)\n",
    "print(\"Neural Network R2 Score: {:.5f}\".format(ann_r2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
