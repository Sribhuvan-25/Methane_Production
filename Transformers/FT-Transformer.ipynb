{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FT-Transformer] Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sb/miniforge3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] Loss: 267.6487\n",
      "Epoch [2/50] Loss: 248.6782\n",
      "Epoch [3/50] Loss: 240.2321\n",
      "Epoch [4/50] Loss: 246.3775\n",
      "Epoch [5/50] Loss: 239.7164\n",
      "Epoch [6/50] Loss: 216.2830\n",
      "Epoch [7/50] Loss: 256.2126\n",
      "Epoch [8/50] Loss: 184.9729\n",
      "Epoch [9/50] Loss: 206.0536\n",
      "Epoch [10/50] Loss: 250.0005\n",
      "Epoch [11/50] Loss: 174.8703\n",
      "Epoch [12/50] Loss: 179.0890\n",
      "Epoch [13/50] Loss: 230.0662\n",
      "Epoch [14/50] Loss: 210.4991\n",
      "Epoch [15/50] Loss: 152.1523\n",
      "Epoch [16/50] Loss: 172.1486\n",
      "Epoch [17/50] Loss: 208.0616\n",
      "Epoch [18/50] Loss: 143.0674\n",
      "Epoch [19/50] Loss: 135.8951\n",
      "Epoch [20/50] Loss: 160.6101\n",
      "Epoch [21/50] Loss: 183.8126\n",
      "Epoch [22/50] Loss: 150.4085\n",
      "Epoch [23/50] Loss: 121.6874\n",
      "Epoch [24/50] Loss: 118.6678\n",
      "Epoch [25/50] Loss: 114.3801\n",
      "Epoch [26/50] Loss: 122.3542\n",
      "Epoch [27/50] Loss: 106.6539\n",
      "Epoch [28/50] Loss: 134.2132\n",
      "Epoch [29/50] Loss: 107.7782\n",
      "Epoch [30/50] Loss: 119.7869\n",
      "Epoch [31/50] Loss: 112.5130\n",
      "Epoch [32/50] Loss: 128.2462\n",
      "Epoch [33/50] Loss: 122.0447\n",
      "Epoch [34/50] Loss: 108.3985\n",
      "Epoch [35/50] Loss: 105.3001\n",
      "Epoch [36/50] Loss: 120.6747\n",
      "Epoch [37/50] Loss: 114.0639\n",
      "Epoch [38/50] Loss: 105.7364\n",
      "Epoch [39/50] Loss: 112.5220\n",
      "Epoch [40/50] Loss: 122.9781\n",
      "Epoch [41/50] Loss: 115.8334\n",
      "Epoch [42/50] Loss: 113.8353\n",
      "Epoch [43/50] Loss: 103.4612\n",
      "Epoch [44/50] Loss: 126.0247\n",
      "Epoch [45/50] Loss: 110.9899\n",
      "Epoch [46/50] Loss: 111.7890\n",
      "Epoch [47/50] Loss: 101.0822\n",
      "Epoch [48/50] Loss: 117.3146\n",
      "Epoch [49/50] Loss: 113.9054\n",
      "Epoch [50/50] Loss: 120.2659\n",
      "Fold 1 Results: MSE = 71.0927, RÂ² = -0.0300\n",
      "\n",
      "[FT-Transformer] Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sb/miniforge3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] Loss: 233.0992\n",
      "Epoch [2/50] Loss: 222.8120\n",
      "Epoch [3/50] Loss: 265.5637\n",
      "Epoch [4/50] Loss: 200.8480\n",
      "Epoch [5/50] Loss: 205.1207\n",
      "Epoch [6/50] Loss: 184.2469\n",
      "Epoch [7/50] Loss: 183.5707\n",
      "Epoch [8/50] Loss: 177.9490\n",
      "Epoch [9/50] Loss: 183.9512\n",
      "Epoch [10/50] Loss: 222.7711\n",
      "Epoch [11/50] Loss: 175.5731\n",
      "Epoch [12/50] Loss: 216.4648\n",
      "Epoch [13/50] Loss: 156.3698\n",
      "Epoch [14/50] Loss: 184.6034\n",
      "Epoch [15/50] Loss: 180.0985\n",
      "Epoch [16/50] Loss: 155.9326\n",
      "Epoch [17/50] Loss: 172.7530\n",
      "Epoch [18/50] Loss: 134.9559\n",
      "Epoch [19/50] Loss: 120.9826\n",
      "Epoch [20/50] Loss: 117.1790\n",
      "Epoch [21/50] Loss: 148.5008\n",
      "Epoch [22/50] Loss: 141.3353\n",
      "Epoch [23/50] Loss: 112.7411\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# -------------------------\n",
    "# Dataset (reuse same as above)\n",
    "# -------------------------\n",
    "target_columns = [\n",
    "    \"Average-Total-ISD-Cells\", \"ACE-Xi\", \"ACE-km\", \"ACE-Ks\", \"H2-Xi\", \"H2-km\", \"H2-Ks\",\n",
    "    \"Digester_BD\", \"Digester_BF\", \"Digester_CB\", \"Digester_CP\", \"Digester_FD\", \"Digester_GB\",\n",
    "    \"Digester_GP\", \"Digester_JB\", \"Digester_LP\", \"Digester_MA\", \"Digester_NB\", \"Digester_NS\",\n",
    "    \"Digester_PC\", \"Digester_PO\", \"Digester_SF\", \"Digester_SS\", \"Digester_SW\", \"Digester_WA\",\n",
    "    \"Digester_WP\", \"Digester_WR\", \"Source_I\", \"Source_M\", \"Source_P\", \"Type_CSTR\", \"Type_EFB\",\n",
    "    \"Type_EGSB\", \"Type_Lagoon\", \"Type_UASB\", \"Waste_BW\", \"Waste_Dairy\", \"Waste_FW\", \"Waste_HSI\",\n",
    "    \"Waste_MPW\", \"Waste_MS\", \"Waste_MS+Dairy\", \"Waste_MS+HSI\", \"Waste_PP\", \"Waste_PR\",\n",
    "    \"Waste_SDW\", \"Biomass_F\", \"Biomass_G\"\n",
    "]\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, df, cat_cols, num_cols, target_col, scaler=None, cat_encoders=None):\n",
    "        self.df = df.copy().dropna().reset_index(drop=True)\n",
    "        self.df = self.df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        self.cat_cols = cat_cols\n",
    "        self.num_cols = num_cols\n",
    "        self.target_col = target_col\n",
    "        \n",
    "        if scaler is None:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.num_data = self.scaler.fit_transform(self.df[num_cols])\n",
    "        else:\n",
    "            self.scaler = scaler\n",
    "            self.num_data = self.scaler.transform(self.df[num_cols])\n",
    "        \n",
    "        self.cat_encoders = {}\n",
    "        cat_data_list = []\n",
    "        for col in cat_cols:\n",
    "            if cat_encoders is None or col not in cat_encoders:\n",
    "                le = LabelEncoder()\n",
    "                encoded = le.fit_transform(self.df[col])\n",
    "                self.cat_encoders[col] = le\n",
    "            else:\n",
    "                le = cat_encoders[col]\n",
    "                self.cat_encoders[col] = le\n",
    "                encoded = le.transform(self.df[col])\n",
    "            cat_data_list.append(encoded.astype(np.int64))\n",
    "        if len(cat_cols) > 0:\n",
    "            self.cat_data = np.stack(cat_data_list, axis=1)\n",
    "        else:\n",
    "            self.cat_data = None\n",
    "        self.target = self.df[target_col].values.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        num = self.num_data[idx]\n",
    "        if self.cat_data is not None:\n",
    "            cat = self.cat_data[idx]\n",
    "            return (torch.tensor(num, dtype=torch.float32),\n",
    "                    torch.tensor(cat, dtype=torch.long),\n",
    "                    torch.tensor(self.target[idx], dtype=torch.float32))\n",
    "        else:\n",
    "            return (torch.tensor(num, dtype=torch.float32),\n",
    "                    torch.tensor(self.target[idx], dtype=torch.float32))\n",
    "\n",
    "# -------------------------\n",
    "# FT-Transformer Model\n",
    "# -------------------------\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, num_num_features, cat_cardinalities, d_token=64, nhead=8,\n",
    "                 num_layers=4, dropout=0.1, mlp_hidden=64):\n",
    "        \"\"\"\n",
    "        num_num_features: Number of numerical features.\n",
    "        cat_cardinalities: List with the number of unique values for each categorical feature.\n",
    "        \"\"\"\n",
    "        super(FTTransformer, self).__init__()\n",
    "        # Project each numerical feature (scalar) to a token\n",
    "        self.num_tokens = nn.ModuleList([nn.Linear(1, d_token) for _ in range(num_num_features)])\n",
    "        # Embedding for each categorical feature\n",
    "        self.cat_tokens = nn.ModuleList([nn.Embedding(card, d_token) for card in cat_cardinalities])\n",
    "        # Total tokens = numerical tokens + categorical tokens\n",
    "        self.total_tokens = len(self.num_tokens) + len(self.cat_tokens)\n",
    "        # Learnable [CLS] token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_token))\n",
    "        # Positional embeddings for entire sequence (CLS + features)\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, self.total_tokens + 1, d_token))\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_token, nhead=nhead, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        # MLP head for regression (using CLS token)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(d_token),\n",
    "            nn.Linear(d_token, mlp_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden, 1)\n",
    "        )\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embedding, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        for layer in self.num_tokens:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            if layer.bias is not None:\n",
    "                nn.init.zeros_(layer.bias)\n",
    "        for emb in self.cat_tokens:\n",
    "            nn.init.uniform_(emb.weight, -0.1, 0.1)\n",
    "    \n",
    "    def forward(self, num_data, cat_data):\n",
    "        batch_size = num_data.size(0)\n",
    "        tokens = []\n",
    "        # Process numerical features individually\n",
    "        for i, proj in enumerate(self.num_tokens):\n",
    "            token = proj(num_data[:, i:i+1])  # [batch, d_token]\n",
    "            tokens.append(token.unsqueeze(1))\n",
    "        # Process categorical features (if provided)\n",
    "        if cat_data is not None:\n",
    "            for i, emb in enumerate(self.cat_tokens):\n",
    "                token = emb(cat_data[:, i])  # [batch, d_token]\n",
    "                tokens.append(token.unsqueeze(1))\n",
    "        tokens = torch.cat(tokens, dim=1)  # [batch, total_tokens, d_token]\n",
    "        # Prepend the [CLS] token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # [batch, 1, d_token]\n",
    "        tokens = torch.cat([cls_tokens, tokens], dim=1)  # [batch, total_tokens+1, d_token]\n",
    "        tokens = tokens + self.pos_embedding  # Add positional embeddings\n",
    "        # Transformer expects (seq_len, batch, d_token)\n",
    "        tokens = tokens.transpose(0, 1)\n",
    "        encoded = self.transformer(tokens)\n",
    "        encoded = encoded.transpose(0, 1)\n",
    "        cls_out = encoded[:, 0, :]  # Use [CLS] token output\n",
    "        out = self.mlp_head(cls_out)\n",
    "        return out.squeeze(1)\n",
    "\n",
    "# -------------------------\n",
    "# Cross-Validation and Training for FT-Transformer\n",
    "# -------------------------\n",
    "def cross_validate_ft(df, target_column, cat_cols, num_cols, k_folds=5,\n",
    "                      epochs=50, batch_size=32, learning_rate=0.001):\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    mse_scores, r2_scores = [], []\n",
    "    all_actuals, all_predictions = [], []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
    "        print(f\"\\n[FT-Transformer] Fold {fold+1}/{k_folds}\")\n",
    "        df_train = df.iloc[train_idx].reset_index(drop=True)\n",
    "        df_val   = df.iloc[val_idx].reset_index(drop=True)\n",
    "        train_ds = TabularDataset(df_train, cat_cols, num_cols, target_column)\n",
    "        val_ds = TabularDataset(df_val, cat_cols, num_cols, target_column,\n",
    "                                scaler=train_ds.scaler, cat_encoders=train_ds.cat_encoders)\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        num_num_features = len(num_cols)\n",
    "        cat_cardinalities = [len(train_ds.cat_encoders[col].classes_) for col in cat_cols]\n",
    "        \n",
    "        model = FTTransformer(num_num_features=num_num_features, cat_cardinalities=cat_cardinalities,\n",
    "                              d_token=64, nhead=8, num_layers=4, dropout=0.1, mlp_hidden=64).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for batch in train_loader:\n",
    "                if len(cat_cols) > 0:\n",
    "                    X_num, X_cat, y = batch\n",
    "                    X_num, X_cat, y = X_num.to(device), X_cat.to(device), y.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    preds = model(X_num, X_cat)\n",
    "                else:\n",
    "                    X_num, y = batch\n",
    "                    X_num, y = X_num.to(device), y.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    preds = model(X_num, None)\n",
    "                loss = criterion(preds, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "        \n",
    "        model.eval()\n",
    "        fold_preds, fold_actuals = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                if len(cat_cols) > 0:\n",
    "                    X_num, X_cat, y = batch\n",
    "                    X_num, X_cat, y = X_num.to(device), X_cat.to(device), y.to(device)\n",
    "                    preds = model(X_num, X_cat)\n",
    "                else:\n",
    "                    X_num, y = batch\n",
    "                    X_num, y = X_num.to(device), y.to(device)\n",
    "                    preds = model(X_num, None)\n",
    "                fold_preds.extend(preds.cpu().numpy())\n",
    "                fold_actuals.extend(y.cpu().numpy())\n",
    "        mse = mean_squared_error(fold_actuals, fold_preds)\n",
    "        r2 = r2_score(fold_actuals, fold_preds)\n",
    "        mse_scores.append(mse)\n",
    "        r2_scores.append(r2)\n",
    "        all_actuals.extend(fold_actuals)\n",
    "        all_predictions.extend(fold_preds)\n",
    "        print(f\"Fold {fold+1} Results: MSE = {mse:.4f}, RÂ² = {r2:.4f}\")\n",
    "    \n",
    "    print(f\"\\n[FT-Transformer] Final CV: Mean MSE = {np.mean(mse_scores):.4f}, Mean RÂ² = {np.mean(r2_scores):.4f}\")\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.regplot(x=all_actuals, y=all_predictions, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "    plt.xlabel(\"Actual Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.title(f\"FT-Transformer: Predicted vs Actual ({target_column})\")\n",
    "    plt.show()\n",
    "\n",
    "# -------------------------\n",
    "# Main Execution for FT-Transformer\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"Data/New_data.csv\")  # Update as needed\n",
    "    target_column = \"ACE-km\"\n",
    "    cat_cols = []  # (Populate if needed)\n",
    "    num_cols = [col for col in df.columns if col not in target_columns]\n",
    "    cross_validate_ft(df, target_column, cat_cols, num_cols,\n",
    "                      k_folds=5, epochs=50, batch_size=32, learning_rate=0.001)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
